{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Average_Word_Embedding.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNAdTSDTs4l0Fn04MDO+B1V"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 문서 임베딩 : 워드 임베딩의 평균"],"metadata":{"id":"ERtG7OE4PzSQ"}},{"cell_type":"markdown","source":["특정 문장 내의 단어들의 임베딩 벡터들의 평균이 그 문장의 벡터가 될 수 있습니다.\n","\n","임베딩이 잘 된 상황에서는 단어 벡터들의 평균만으로 텍스트 분류를 수행할 수 있습니다."],"metadata":{"id":"Ek5EtXRwPz5-"}},{"cell_type":"markdown","source":["## 데이터 로드와 전처리"],"metadata":{"id":"nITSPlKxQ4gC"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"AsM7ceUmPiK6","executionInfo":{"status":"ok","timestamp":1643249388480,"user_tz":-540,"elapsed":2793,"user":{"displayName":"임성국","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16836461090630312519"}}},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing.sequence import pad_sequences"]},{"cell_type":"markdown","source":["케라스에서는 imdb.data_load()를 통해서 IMDB 리뷰 데이터를 다운로드 할 수 있는데, 데이터를 로드할 때 파라미터로 num_words를 사용하면 이 데이터에서 등장 빈도 순위로 몇 번째에 해당하는 단어까지를 사용할 것인지를 의미합니다. 만약 vocab_size를 20,000으로 지정할 경우 등장 빈도 순위가 20,000등이 넘는 단어들은 데이터를 로드할 때 전부 제거 후 로드합니다."],"metadata":{"id":"ouG_fOw7Q9x_"}},{"cell_type":"code","source":["vocab_size = 20000\n","\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocab_size)\n","\n","print('훈련용 리뷰 개수 :', len(X_train))\n","print('테스트용 리뷰 개수 :', len(X_test))            "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VG2jrHvLQVqR","executionInfo":{"status":"ok","timestamp":1643249392272,"user_tz":-540,"elapsed":3795,"user":{"displayName":"임성국","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16836461090630312519"}},"outputId":"4d9cf91f-e116-40de-f4e2-799664a5e82d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17465344/17464789 [==============================] - 0s 0us/step\n","17473536/17464789 [==============================] - 0s 0us/step\n","훈련용 리뷰 개수 : 25000\n","테스트용 리뷰 개수 : 25000\n"]}]},{"cell_type":"markdown","source":["이 데이터는 이미 정수 인코딩까지의 전처리가 진행되어져 있습니다. 그래서 단어 집합을 만들고, 각 단어를 정수로 인코딩하는 과정을 할 필요가 없습니다."],"metadata":{"id":"Tr5CeLoHRCyv"}},{"cell_type":"code","source":["print('훈련 데이터의 첫번째 샘플 :',X_train[0])\n","print('훈련 데이터의 첫번째 샘플의 레이블 :', y_train[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ZUWw3B5Q2lV","executionInfo":{"status":"ok","timestamp":1643249392272,"user_tz":-540,"elapsed":4,"user":{"displayName":"임성국","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16836461090630312519"}},"outputId":"3bf7b99c-cb70-48f2-85dd-dd5e535c111d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련 데이터의 첫번째 샘플 : [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n","훈련 데이터의 첫번째 샘플의 레이블 : 1\n"]}]},{"cell_type":"code","source":["print('훈련용 리뷰의 평규 길이: {}'.format(np.mean(list(map(len, X_train)), dtype=int)))\n","print('테스트용 리뷰의 평균 길이: {}'.format(np.mean(list(map(len, X_test)), dtype=int)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iUdwgInsREW7","executionInfo":{"status":"ok","timestamp":1643249392273,"user_tz":-540,"elapsed":4,"user":{"displayName":"임성국","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16836461090630312519"}},"outputId":"3221f8ed-df43-4cd6-a42b-d45fb78bd8ae"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["훈련용 리뷰의 평규 길이: 238\n","테스트용 리뷰의 평균 길이: 230\n"]}]},{"cell_type":"markdown","source":["훈련용 리뷰와 테스트용 리뷰의 평균 길이가 각각 238, 230입니다. 평균보다는 큰 수치인 400으로 패딩합니다."],"metadata":{"id":"B_txOUrWRPZd"}},{"cell_type":"code","source":["max_len = 400\n","\n","X_train = pad_sequences(X_train, maxlen=max_len)\n","X_test = pad_sequences(X_test, maxlen=max_len)\n","print('X_train의 크기(shape) :', X_train.shape)\n","print('X_test의 크기(shape) :', X_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eGzEA5JjRLp5","executionInfo":{"status":"ok","timestamp":1643249393360,"user_tz":-540,"elapsed":1090,"user":{"displayName":"임성국","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16836461090630312519"}},"outputId":"deccdc91-6adb-49e3-e752-20637d25f543"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["X_train의 크기(shape) : (25000, 400)\n","X_test의 크기(shape) : (25000, 400)\n"]}]},{"cell_type":"markdown","source":["# 모델 설계하기"],"metadata":{"id":"cNNuYnrvRUCa"}},{"cell_type":"markdown","source":["모델의 입력으로 사용하기 위한 모든 전처리를 마쳤습니다. 임베딩 벡터를 평균으로 사용하는 모델을 설계해봅시다. GlobalAveragePooling1D()은 입력으로 들어오는 모든 벡터들의 평균을 구하는 역할을 합니다. Embedding() 다음에 GlobalAveragePooling1D()을 추가하면 해당 문장의 모든 단어 벡터들의 평균 벡터를 구합니다.\n","\n","이진 분류를 수행해야 하므로 그 후에는 시그모이드 함수를 활성화 함수로 사용하는 뉴런 1개를 배치합니다. 훈련 데이터의 20%를 검증 데이터로 사용하고 총 10 에포크 학습합니다."],"metadata":{"id":"Tf6bCd3dRnM9"}},{"cell_type":"code","source":["from tensorflow.keras.models import Sequential, load_model\n","from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","embedding_dim = 64\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size, embedding_dim))\n","\n","# 모든 단어 벡터의 평균을 구한다.\n","model.add(GlobalAveragePooling1D())\n","model.add(Dense(1, activation='sigmoid'))\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n","mc = ModelCheckpoint('embedding_average_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n","model.fit(X_train, y_train, batch_size=32, epochs=10, callbacks=[es,mc], validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sD0AJf32RSWq","executionInfo":{"status":"ok","timestamp":1643249436913,"user_tz":-540,"elapsed":42178,"user":{"displayName":"임성국","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16836461090630312519"}},"outputId":"17d13c43-8d51-410c-d941-209f74413eb6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","625/625 [==============================] - ETA: 0s - loss: 0.6245 - acc: 0.7235\n","Epoch 00001: val_acc improved from -inf to 0.81500, saving model to embedding_average_model.h5\n","625/625 [==============================] - 7s 7ms/step - loss: 0.6245 - acc: 0.7235 - val_loss: 0.5183 - val_acc: 0.8150\n","Epoch 2/10\n","620/625 [============================>.] - ETA: 0s - loss: 0.4260 - acc: 0.8570\n","Epoch 00002: val_acc improved from 0.81500 to 0.86900, saving model to embedding_average_model.h5\n","625/625 [==============================] - 3s 5ms/step - loss: 0.4252 - acc: 0.8572 - val_loss: 0.3764 - val_acc: 0.8690\n","Epoch 3/10\n","609/625 [============================>.] - ETA: 0s - loss: 0.3168 - acc: 0.8907\n","Epoch 00003: val_acc improved from 0.86900 to 0.88160, saving model to embedding_average_model.h5\n","625/625 [==============================] - 3s 5ms/step - loss: 0.3160 - acc: 0.8910 - val_loss: 0.3219 - val_acc: 0.8816\n","Epoch 4/10\n","625/625 [==============================] - ETA: 0s - loss: 0.2599 - acc: 0.9097\n","Epoch 00004: val_acc improved from 0.88160 to 0.88540, saving model to embedding_average_model.h5\n","625/625 [==============================] - 4s 6ms/step - loss: 0.2599 - acc: 0.9097 - val_loss: 0.2970 - val_acc: 0.8854\n","Epoch 5/10\n","624/625 [============================>.] - ETA: 0s - loss: 0.2225 - acc: 0.9234\n","Epoch 00005: val_acc improved from 0.88540 to 0.88760, saving model to embedding_average_model.h5\n","625/625 [==============================] - 4s 7ms/step - loss: 0.2225 - acc: 0.9233 - val_loss: 0.2833 - val_acc: 0.8876\n","Epoch 6/10\n","617/625 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.9352\n","Epoch 00006: val_acc improved from 0.88760 to 0.89300, saving model to embedding_average_model.h5\n","625/625 [==============================] - 4s 7ms/step - loss: 0.1937 - acc: 0.9350 - val_loss: 0.2746 - val_acc: 0.8930\n","Epoch 7/10\n","624/625 [============================>.] - ETA: 0s - loss: 0.1704 - acc: 0.9437\n","Epoch 00007: val_acc did not improve from 0.89300\n","625/625 [==============================] - 4s 6ms/step - loss: 0.1703 - acc: 0.9437 - val_loss: 0.2747 - val_acc: 0.8928\n","Epoch 8/10\n","622/625 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9515\n","Epoch 00008: val_acc improved from 0.89300 to 0.89520, saving model to embedding_average_model.h5\n","625/625 [==============================] - 3s 5ms/step - loss: 0.1499 - acc: 0.9516 - val_loss: 0.2739 - val_acc: 0.8952\n","Epoch 9/10\n","610/625 [============================>.] - ETA: 0s - loss: 0.1334 - acc: 0.9580\n","Epoch 00009: val_acc improved from 0.89520 to 0.89680, saving model to embedding_average_model.h5\n","625/625 [==============================] - 3s 5ms/step - loss: 0.1335 - acc: 0.9579 - val_loss: 0.2749 - val_acc: 0.8968\n","Epoch 10/10\n","620/625 [============================>.] - ETA: 0s - loss: 0.1175 - acc: 0.9634\n","Epoch 00010: val_acc did not improve from 0.89680\n","625/625 [==============================] - 3s 5ms/step - loss: 0.1177 - acc: 0.9634 - val_loss: 0.2803 - val_acc: 0.8948\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f820ddf4b10>"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["loaded_model = load_model('embedding_average_model.h5')\n","print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RgXOVhzrR9Nf","executionInfo":{"status":"ok","timestamp":1643249478191,"user_tz":-540,"elapsed":5573,"user":{"displayName":"임성국","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16836461090630312519"}},"outputId":"47bac5dc-db77-4bd3-a755-8cbcc8483b21"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["782/782 [==============================] - 3s 3ms/step - loss: 0.2863 - acc: 0.8874\n","\n"," 테스트 정확도: 0.8874\n"]}]}]}